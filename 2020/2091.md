---
author: owent
categories:
  - Article
  - Blablabla
date: 2020-02-10 17:12:00
draft: true
id: 2091
tags: 
  - 事务
  - 分布式
  - 分布式事务
  - distribute
  - transaction
  - distributed transaction
  - percolator
  - spanner
title: 在游戏服务器中使用分布式事务
type: post
---

前言
----------------------------------------------

游戏业务通常有个特点是模块相关性非常高，模块之间的联动也非常密集且复杂。要保持各个相关模块的数据一致性，同时又兼顾效率和，没有一个通用的方法。通常的做法是走有损服务（也叫柔性服务）和自动修复的方式。比如支付服务一般的做法是在2PC的基础上增加redo log，对于发放和订单确认这两方，如果失败了会尝试几次补发。又或者好友系统或者公会，因为涉及多个对象的数据相互索引，一些做法是玩家在线的时候定期去检查数据是否正确，如果不正确走修复流程。

我去年的时候看了下Google的一些分布式系统的分布式事务的设计的论文（[《Google去中心化分布式系统论文三件套(Percolator、Spanner、F1)读后感》][2]），感觉上也挺适合游戏业务中某些系统的使用场景。所以在我们现在的项目中就做了一些尝试，对一些系统引入了分布式事务的设计，主要目的还是为了解决数据一致性，并且提供可大规模平行扩容的能力。

基本流程和通用分布式事务服务器
----------------------------------------------

其实分布式事务的本质是把事务的成功与否收敛到单点上，也就是协调者。

TCC

1. 创建事务,分配事务ID（两次数据库操作，一次分配ID，一次创建）
2. 所有参与者预提交阶段(Try，1次RPC)
3. 事务提交（Commit/Cancel，1次RPC+1次数据库操作）
4. 通知所有参与者提交成功/失败（2次RPC）
5. 所有参与者确认提交成功后删除事务（数据库操作）

容灾和负载均衡
----------------------------------------------

1. 参与者再超时时去事务服务器拉取事务状态
  > 1.1 Not  Found
  > 1.2 Commited
  > 1.2 Abort


业务补偿： event_id 

幂等性实现： f(f(x)) = f(x)


垃圾处理
----------------------------------------------

比较
----------------------------------------------

|           特性      |                    我们的分布式事务服务                  |               Google Percolator                  |               Google Spanner                        |
|---------------------|----------------------------------------------------------|--------------------------------------------------|-----------------------------------------------------|
| 事件ID/事件版本号   | 池化分配，按服务对象key分布式事件版本号,设计QPS>1亿/s    | 全服单点timestamp oracle服务，QPS大约200万/s     | TrueTime时间戳服务                                  |
| 协调者              | 专用协调者服务器                                         | 固定算法，指定一个参与事务执行者做协调者         | Paxos算法选举Leader做协调者                         |
| 脏数据/垃圾清理     | 协调者服务器定期清理，下一次GET/SET时清理                | 下一次GET/SET时清理                              | 下一次GET/SET时清理                                 |
| 底层存储            | 任意内存数据库(比如redis)                                | BigTable                                         | BigTable                                            |
| 延迟                | 每个事务4次DB请求，1次内部RPC，每个参与者3次内部RPC      | 索引服务，延迟不敏感                             | 26-200ms                                            |
| QPS                 | 依赖服务器间通信和底层存储，设计QPS>=1亿/s               | 对比BigTable，读事务\*0.94，写事务BigTable\*0.23 | -                                                   |
| 冲突策略            | 根据业务实现，目前业务实现是无锁。[Wound-wait][1]        | 悲观锁，类似 [Wound-wait][1]                     | 只读无锁，读写悲观锁，[Wound-wait]（F1引入了乐观锁）|
| 其他特点            | 适用于多参与者对等接入，适用于多种业务类型，但接入较复杂 | 索引服务，自动化依赖分析和触发依赖事务           | 分布式关系型数据库                                  |

优化
----------------------------------------------

垃圾清理通知


[^paxos]: https://en.wikipedia.org/wiki/Paxos_(computer_science) "Paxos"
[^chubby]: https://ai.google/research/pubs/pub27897 "The Chubby lock service for loosely-coupled distributed systems"
[^gfs]: https://ai.google/research/pubs/pub51 "The Google File System"
[^bigtable]: https://ai.google/research/pubs/pub27898 "Bigtable: A Distributed Storage System for Structured Data "
[^percolator]: https://ai.google/research/pubs/pub36726 "Large-scale Incremental Processing Using Distributed Transactions and Notifications"
[^spanner]: https://ai.google/research/pubs/pub39966 "Spanner: Google's Globally-Distributed Database"
[^f1]: https://ai.google/research/pubs/pub41344 "F1: A Distributed SQL Database That Scales"
[^redis]: https://redis.io "Redis"
[^raft]: https://raft.github.io/ "The Raft Consensus Algorithm"

[1]: https://en.wikibooks.org/wiki/Design_of_Main_Memory_Database_System/Concurrency#8.3.4.1_Dead_Lock_Prevention
[2]: https://owent.net/2019/1902.html
